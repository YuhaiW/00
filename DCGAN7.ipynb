{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMrG4rXQephas4FcdfGq5jl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YuhaiW/00/blob/main/DCGAN7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jTkUJGGdL6eU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision \n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms \n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules.activation import LeakyReLU\n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, channel_img, features_d):\n",
        "    super(Discriminator, self).__init__()\n",
        "    # input = N * channel_img * 64 * 64\n",
        "    self.disc = nn.Sequential(\n",
        "        nn.Conv2d(\n",
        "            channel_img,\n",
        "            features_d,\n",
        "            kernel_size = 4,\n",
        "            stride = 2,\n",
        "            padding = 1\n",
        "        ), # 32 * 32\n",
        "        nn.LeakyReLU(0.2),\n",
        "        self._block(features_d, features_d*2, 4, 2, 1),  # 16 *16\n",
        "        self._block(features_d*2, features_d*4, 4, 2, 1), # 8 * 8\n",
        "        self._block(features_d*4, features_d*8, 4, 2, 1), # 4 * 4\n",
        "        nn.Conv2d(features_d*8, 1, 4, 2, 0),  #1 * 1\n",
        "        nn.Sigmoid(),\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def _block(self, in_channel, out_channel, kernel_size, stride, padding):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels = in_channel, \n",
        "                  out_channels = out_channel, \n",
        "                  kernel_size = kernel_size,\n",
        "                  stride = stride,\n",
        "                  padding = padding,\n",
        "                  bias = False), # cause we will use \"BatchNorm2d\", so the bias = False\n",
        "        nn.BatchNorm2d(num_features = out_channel),\n",
        "        nn.LeakyReLU(0.2),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.disc(x)"
      ],
      "metadata": {
        "id": "iX0MbZO6Ngj1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules.conv import ConvTranspose2d\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self, z_dim, channels_img, feature_g):\n",
        "    super(Generator, self).__init__()\n",
        "    self.gen = nn.Sequential(\n",
        "    self._block(z_dim, feature_g*16, 4, 1, 0),\n",
        "    self._block(z_dim*16, feature_g*8, 4, 2, 1),\n",
        "    self._block(z_dim*8, feature_g*4, 4, 2, 1),\n",
        "    self._block(z_dim*4, feature_g*2, 4, 2, 1),\n",
        "    nn,ConvTranspose2d(feature_g*2, channels_img, 4, 2, 1),\n",
        "    nn.Tanh(), #[-1,1]\n",
        "    )\n",
        "\n",
        "\n",
        "  def _block(self, in_channel, out_channel, kernel_size, stride, padding):\n",
        "    return nn.Sequential(\n",
        "         nn.ConvTranspose2d(in_channels = in_channel, \n",
        "                  out_channels = out_channel, \n",
        "                  kernel_size = kernel_size,\n",
        "                  stride = stride,\n",
        "                  padding = padding,\n",
        "                  bias = False), # cause we will use \"BatchNorm2d\", so the bias = False\n",
        "        nn.BatchNorm2d(num_features = out_channel),\n",
        "        nn.LeakyReLU(0.2),\n",
        "    )\n",
        "  \n",
        "def forward(self, x):\n",
        "    return self.gen(x)"
      ],
      "metadata": {
        "id": "kGybjhgua4uW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_weight(model):\n",
        "  for m in model.modules():\n",
        "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
        "      nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "      "
      ],
      "metadata": {
        "id": "SNZW1zJkhZhQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2wxOZeieoHBQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}